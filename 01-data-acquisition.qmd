---
title: 01-data-acquisition
author: Gayoung Kim
format: 
    html:
        code-fold: false
        toc: true
        toc-depth: 3
---

```{python}
#| code-fold: true
#| message: false
#| output: false

# setup 
# import packages
import json
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple
import re
from collections import defaultdict
from tqdm import tqdm

import warnings
warnings.filterwarnings('ignore')

import os
import requests
from xml.etree import ElementTree as ET
```

```{python}
# Configuration
RAW_DATA_DIR = Path("data/raw")
## for the sake of final project assignment - you can test redownloading the data by changing/creating a new directory 
OUTPUT_DIR = Path("data/processed")
TEMP_DIR = Path("data/temp")

# Create directories if directories do not exist
for dir_path in [OUTPUT_DIR, TEMP_DIR, RAW_DATA_DIR]:
    dir_path.mkdir(parents=True, exist_ok=True)
```

## Overview

This notebook acquires data from three primary sources:

1.  **Bioguide profiles** - Biographical information for all members of Congress
2.  **Bill data** - Detailed information about bills, sponsorships, and legislative actions
3.  **Supplementary datasets** - Legislative effectiveness scores and historical legislator records

These data sources share a common identifier (`bioguide_id`) that enables linking across datasets.

# Bioguide

### Why Bioguide?

The [Congressional Bioguide](https://bioguide.congress.gov/) provides authoritative biographical data directly from Congress. Unlike other sources, it includes standardized identifiers (e.g., `usCongressBioId`) that facilitate data linkage across our analysis pipeline. The bulk download format allows us to work with local copies rather than making repeated API calls.

## Load Data

Profiles of all congressmembers (and important political figure such as President and Vice president) in the United States Congress is accessed as a bulk data and downloaded manually from [biguide.congress.gov](https://bioguide.congress.gov/) in a compressed `.zip` format. (`data/raw/BioguideProfiles.zip`) (Accessed Septempber 18th, 2025)

It is manually unzipped and stored as `.json` files in the `data/raw/BioguideProfiles` folder.

```{python}
def load_all_bioguide_files(directory: Path):
    # Load all bioguide JSON files from a directory.
    
    all_records = []
    json_files = list(directory.glob("*.json"))
    
    print(f"Found {len(json_files)} JSON files")
    
    for json_file in tqdm(json_files, desc="Loading files"):
        with open(json_file, 'r', encoding='utf-8') as f:
            content = f.read().strip()
            parsed = json.loads(content)
            
            # Check if wrapped in "data" key
            if isinstance(parsed, dict) and 'data' in parsed:
                parsed = parsed['data']
            
            # Convert to list if single object
            if isinstance(parsed, dict):
                records = [parsed]
            elif isinstance(parsed2, list):
                records = parsed
            else:
                print(f"Warning: Unexpected JSON structure in {json_file.name}")
        
        # Add filename to each record
        filename = json_file.stem  
        for record in records:
            record['source_file_id'] = filename
        
        all_records.extend(records)
    
    df = pd.DataFrame(all_records)
    
    print(f"Total records after cleaning: {len(df)}")
    return df
```

```{python}
# load the data
BIO_RAW_DIR = Path("data/raw/BioguideProfiles")
bio_df = load_all_bioguide_files(BIO_RAW_DIR)
```

```{python}
print(f"Total records loaded: {len(bio_df)}")
print(f"Unique bioguide IDs: {bio_df['usCongressBioId'].nunique()}")
print(f"Duplicate check: {bio_df['usCongressBioId'].duplicated().sum()} duplicates")
print(f"\nColumns available: {bio_df.columns.tolist()}")
```

## Basic Cleaning

```{python}
# check column name
bio_df.columns
```

### Missing Data

```{python}
# check na
print(f"{bio_df['usCongressBioId'].isna().sum()} files have invalid data.")

# clean invalid records
if bio_df['usCongressBioId'].isna().sum() > 0 :
   bio_df = bio_df[bio_df['usCongressBioId'].notna()]
```

### save temporary data

```{python}
bio_df.to_pickle(TEMP_DIR / 'bioguide_raw.pkl')
print(f"\nSaved raw data to {TEMP_DIR / 'bioguide_raw.pkl'}")
```

# Bills

Bill Data Download Strategy The GovInfo API provides bulk bill data in XML format. I download:

-   **Bill status files** (not full text) to keep file sizes manageable

    -   with **Specific bill types** to focus on primary legislation (HR bills)

        -   Common types are 'hr' (House Bill), 's' (Senate Bill), 'hjres' (House Joint Resolution), 'sjres' (Senate Joint Resolution), 'hconres' (House Concurrent Resolution), 'sconres' (Senate Concurrent Resolution)

    -   for **Recent sessions only** (117th, 118th) for computational feasibility

The download function includes checks to handle interrupted downloads and errors.

## Functions

### Download bill bulk data

```{python}
def download_congress_bills(
    session,
    bill_types,
    output_dir = "./data/raw/bills"
) :
    # Download bill XML files from govinfo.gov
    
    # Create output directory
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    downloaded_files = {}
    
    for bill_type in bill_types:
        print(f"\n{'='*60}")
        print(f"Processing {bill_type.upper()} bills from session {session}")
        print(f"{'='*60}")
        
        # Create subdirectory for this bill type
        bill_type_dir = output_path / f"{session}" / bill_type
        bill_type_dir.mkdir(parents=True, exist_ok=True)
        
        # Get list of bills from JSON endpoint
        # Note: The bill status file list is stored at /BILLSTATUS/session/bill_type 
        # Note: The entire bills structure can be downloaded from /BILLS/session/half/bill_type for the listing (half can be 1 or two)

        json_url = f"https://www.govinfo.gov/bulkdata/json/BILLSTATUS/{session}/{bill_type}"
        print(f"Fetching bill list from: {json_url}")
        
        headers = {'Accept': 'application/json'}

        try:
            response = requests.get(json_url, headers=headers, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            files = data.get('files', [])
            
            if not files:
                print(f"No bills found for {bill_type}")
                continue
            
            print(f"Found {len(files)} bill files")
            
            # Filter for XML files only
            xml_files = [f for f in files if f.get('fileExtension') == 'xml']
            print(f"Downloading {len(xml_files)} XML files...")
            
            type_downloaded = []
            
            # Download each XML file
            for file_info in tqdm(xml_files, desc=f"Downloading {bill_type}"):
                file_url = file_info['link']
                file_name = file_info['name']
                file_path = bill_type_dir / file_name
                
                # Skip if already downloaded
                if file_path.exists():
                    type_downloaded.append(str(file_path))
                    continue
                
                try:
                    file_response = requests.get(file_url, timeout=30)
                    file_response.raise_for_status()
                    
                    with open(file_path, 'wb') as f:
                        f.write(file_response.content)
                    
                    type_downloaded.append(str(file_path))
                    
                except requests.RequestException as e:
                    print(f"\nError downloading {file_name}: {e}")
                    continue
            
            downloaded_files[bill_type] = type_downloaded
            print(f"Successfully downloaded {len(type_downloaded)} files for {bill_type}")
            
        except requests.RequestException as e:
            print(f"Error fetching bill list for {bill_type}: {e}")
            continue

    return downloaded_files
```

```{python}
# backup code to get the list of file paths as "downloaded_files", in case lost connection and values are lost - but do not need to redownload from the web source by using the above function

# from pathlib import Path

# # Scan for already downloaded files
# base_path = Path("./data/raw/bills/118")
# downloaded_files = {}

# for xml_file in base_path.rglob("*.xml"):
#     bill_type = xml_file.parent.name  # Get the bill_type folder name
    
#     if bill_type not in downloaded_files:
#         downloaded_files[bill_type] = []
    
#     downloaded_files[bill_type].append(str(xml_file))

# # Check what you found
# for bill_type, files in downloaded_files.items():
#     print(f"{bill_type}: {len(files)} files")
```

### Data Processing

The XML parsing extracts data into relational tables:

-   bills - One row per bill with metadata

-   sponsorships - One row per person-bill relationship

-   actions - One row per legislative action (action-bill relationship)

-   committees - Committee assignments

-   people - Unique legislator registry

This normalized structure enables efficient querying and prevents data duplication.

```{python}
def parse_bill_xml(xml_path: str, policy_areas: Optional[List[str]] = None):
    """
    Parse a single billStatus XML file and extract all relevant information
    """
    
    try:
        tree = ET.parse(xml_path)
        root = tree.getroot()
        
        # Navigate to the bill element (billStatus structure)
        bill = root.find('.//bill')
        if bill is None:
            print(f"Warning: No bill element found in {xml_path}")
            return None
        
        # Helper function to safely get text
        def get_text(element, xpath, default=""):
            node = element.find(xpath)
            return node.text if node is not None and node.text else default
        
        # Helper function to safely parse date
        def parse_date(date_str):
            if not date_str:
                return None
            try:
                return datetime.strptime(date_str, "%Y-%m-%d").date()
            except:
                return None
        
        # Helper function to safely parse datetime
        def parse_datetime(dt_str):
            if not dt_str:
                return None
            try:
                return datetime.strptime(dt_str, "%Y-%m-%dT%H:%M:%SZ")
            except:
                return None
        
        # Helper function to clean HTML tags
        def clean_html(text):
            if not text:
                return ""
            import re
            return re.sub('<.*?>', '', text).strip()
        
        # Check policy area filter
        policy_area = get_text(bill, ".//policyArea/name").lower()
        if not policy_area:
            policy_area = None
        
        if policy_areas and policy_areas != ['all'] and policy_area not in [pa.lower() for pa in policy_areas]:
            return None
        
        # Extract bill ID
        bill_type = get_text(bill, "type")
        bill_number = get_text(bill, "number")
        congress = get_text(bill, "congress")
        bill_id = f"{bill_type}{bill_number}"
        
        # === EXTRACT BILL-LEVEL INFORMATION ===
        bill_info = {
            'bill_id': bill_id,
            'congress': int(get_text(root, ".//congress", "0")),
            'bill_type': bill_type,
            'bill_number': int(bill_number) if bill_number else 0,
            'origin_chamber': get_text(root, ".//originChamber"),
            'origin_chamber_code': get_text(root, ".//originChamberCode"),
            'introduced_date': parse_date(get_text(root, ".//introducedDate")),
            'update_date': parse_datetime(get_text(root, ".//updateDate")),
            'title': get_text(root, ".//title"),
            'policy_area': policy_area,
            'latest_action_date': parse_date(get_text(root, ".//latestAction/actionDate")),
            'latest_action_text': get_text(root, ".//latestAction/text"),
        }
        
        # === EXTRACT ACTIONS ===
        actions = []
        for action in root.findall(".//actions/item"):
            actions.append({
                'bill_id': bill_id,
                'action_date': parse_date(get_text(action, "actionDate")),
                'action_time': get_text(action, "actionTime"),
                'action_text': get_text(action, "text"),
                'action_type': get_text(action, "type"),
                'action_code': get_text(action, "actionCode"),
                'source_system': get_text(action, "sourceSystem/name"),
                'source_code': get_text(action, "sourceSystem/code"),
            })
        
        # === EXTRACT SPONSORS ===
        sponsors = []
        for sponsor in root.findall(".//sponsors/item"):
            sponsors.append({
                'bill_id': bill_id,
                'bioguide_id': get_text(sponsor, "bioguideId"),
                'full_name': get_text(sponsor, "fullName"),
                'first_name': get_text(sponsor, "firstName"),
                'last_name': get_text(sponsor, "lastName"),
                'party': get_text(sponsor, "party"),
                'state': get_text(sponsor, "state"),
                'district': get_text(sponsor, "district"),
                'is_by_request': get_text(sponsor, "isByRequest"),
                'role': 'sponsor',
                'sponsor_date': bill_info['introduced_date'],
                'sponsorship_withdrawn_date': None,
                'is_original_cosponsor': None,
            })
        
        # === EXTRACT COSPONSORS ===
        cosponsors = []
        for cosponsor in root.findall(".//cosponsors/item"):
            cosponsors.append({
                'bill_id': bill_id,
                'bioguide_id': get_text(cosponsor, "bioguideId"),
                'full_name': get_text(cosponsor, "fullName"),
                'first_name': get_text(cosponsor, "firstName"),
                'last_name': get_text(cosponsor, "lastName"),
                'party': get_text(cosponsor, "party"),
                'state': get_text(cosponsor, "state"),
                'district': get_text(cosponsor, "district"),
                'is_by_request': None,
                'role': 'cosponsor',
                'sponsor_date': parse_date(get_text(cosponsor, "sponsorshipDate")),
                'sponsorship_withdrawn_date': parse_date(get_text(cosponsor, "sponsorshipWithdrawnDate")),
                'is_original_cosponsor': get_text(cosponsor, "isOriginalCosponsor"),
            })
        
        # === EXTRACT COMMITTEES ===
        committees = []
        for committee in root.findall(".//committees/item"):
            committees.append({
                'bill_id': bill_id,
                'committee_code': get_text(committee, "systemCode"),
                'committee_name': get_text(committee, "name"),
                'chamber': get_text(committee, "chamber"),
            })
        
        # Update bill summary statistics
        bill_info['total_sponsors'] = len(sponsors)
        bill_info['total_cosponsors'] = len(cosponsors)
        bill_info['total_actions'] = len(actions)
        
        return {
            'bill': bill_info,
            'actions': actions,
            'sponsors': sponsors,
            'cosponsors': cosponsors,
            'committees': committees,
        }
        
    except Exception as e:
        print(f"Error parsing {xml_path}: {e}")
        return None
```

```{python}
def process_bills_to_dataframes(
    downloaded_files: Dict[str, List[str]],
    policy_areas: Optional[List[str]] = None,
    output_format: str = "flat"
) -> Dict[str, pd.DataFrame]:
    """
    Process downloaded bill XML files into structured dataframes
    
    Parameters:
    -----------
    downloaded_files : Dict[str, List[str]]
        Dictionary mapping bill_type to list of file paths (from download_congress_bills)
    policy_areas : Optional[List[str]]
        Filter bills by policy areas. If None or ['all'], include all bills.
    output_format : str
        'flat' for separate dataframes, 'nested' for bills with nested data
    
    Returns:
    --------
    Dict[str, pd.DataFrame]
        Dictionary containing processed dataframes:
        - bills: Bill-level information
        - actions: All actions taken on bills
        - people: Unique registry of legislators
        - sponsorships: Bill-person sponsorship relationships
        - committees: Committee assignments
        - bill_people_detailed: Full detail with bill connections
    """
    
    all_bills = []
    all_actions = []
    all_people = []
    all_committees = []
    
    # Flatten file list
    all_files = []
    for bill_type, files in downloaded_files.items():
        all_files.extend(files)
    
    print(f"\nProcessing {len(all_files)} bill files...")
    
    # Process each file
    for xml_path in tqdm(all_files, desc="Processing bills"):
        parsed_data = parse_bill_xml(xml_path, policy_areas)
        
        if parsed_data is None:
            continue
        
        all_bills.append(parsed_data['bill'])
        all_actions.extend(parsed_data['actions'])
        all_people.extend(parsed_data['sponsors'])
        all_people.extend(parsed_data['cosponsors'])
        all_committees.extend(parsed_data['committees'])
    
    # Convert to dataframes
    print("\nConverting to dataframes...")
    
    bills_df = pd.DataFrame(all_bills)
    actions_df = pd.DataFrame(all_actions)
    people_df = pd.DataFrame(all_people)
    committees_df = pd.DataFrame(all_committees)
    
    
    # Create unique people registry
    unique_people = people_df[
        ['bioguide_id', 'full_name', 'first_name', 'last_name', 'party', 'state', 'district']
    ].drop_duplicates(subset=['bioguide_id']).reset_index(drop=True)
    
    # Create sponsorship network data
    sponsorship_network = people_df[
        ['bill_id', 'bioguide_id', 'role', 'sponsor_date', 
         'sponsorship_withdrawn_date', 'is_original_cosponsor']
    ].sort_values(['bill_id', 'sponsor_date']).reset_index(drop=True)
    
    print(f"\nProcessing complete!")
    print(f"Number of files to process: {len(all_files)}")
    print(f"Bills extracted: {len(all_bills)}")
    print(f"People records: {len(all_people)}")
    print(f"People_df shape: {people_df.shape}")
    print(f"People_df columns: {people_df.columns.tolist()}")

    print(f"  Bills: {len(bills_df)}")
    print(f"  Unique legislators: {len(unique_people)}")
    print(f"  Total actions: {len(actions_df)}")
    print(f"  Total sponsorships: {len(sponsorship_network)}")
    print(f"  Committee assignments: {len(committees_df)}")
    
    return {
        'bills': bills_df,
        'people': unique_people,
        'actions': actions_df,
        'sponsorships': sponsorship_network,
        'committees': committees_df,
        'bill_people_detailed': people_df
    }
```

```{python}
def generate_summary_stats(data_dict: Dict[str, pd.DataFrame]) -> None:
    """
    Generate and print summary statistics from processed data
    """
    
    bills_df = data_dict['bills']
    sponsorships_df = data_dict['sponsorships']
    people_df = data_dict['people']
    actions_df = data_dict['actions']
    
    print("=" * 80)
    print("CONGRESSIONAL BILLS DATA SUMMARY")
    print("=" * 80)
    
    # === BILLS SUMMARY ===
    print("\nðŸ“‹ BILLS OVERVIEW")
    print("-" * 80)
    print(f"Total Bills: {len(bills_df):,}")
    print(f"Average Sponsors per Bill: {bills_df['total_sponsors'].mean():.2f}")
    print(f"Average Cosponsors per Bill: {bills_df['total_cosponsors'].mean():.2f}")
    
    print("\n  Bills by Type:")
    for bill_type, count in bills_df['bill_type'].value_counts().items():
        print(f"    â€¢ {bill_type}: {count:,} bills")
    
    print("\n  Top 10 Policy Areas:")
    for i, (policy_area, count) in enumerate(bills_df['policy_area'].value_counts().head(10).items(), 1):
        print(f"    {i:2d}. {policy_area}: {count:,} bills")
    
    # === TEMPORAL SUMMARY ===
    print("\nðŸ“… TEMPORAL INFORMATION")
    print("-" * 80)
    date_min = bills_df['introduced_date'].min()
    date_max = bills_df['introduced_date'].max()
    print(f"Date Range: {date_min} to {date_max}")
    
    if date_min and date_max:
        days_span = (date_max - date_min).days
        print(f"Time Span: {days_span:,} days ({days_span/365.25:.1f} years)")
    
    print(f"Total Actions Recorded: {len(actions_df):,}")
    print(f"Average Actions per Bill: {len(actions_df) / len(bills_df):.1f}")
    
    # === PEOPLE SUMMARY ===
    print("\nðŸ‘¥ LEGISLATORS")
    print("-" * 80)
    print(f"Unique Legislators: {len(people_df):,}")
    
    print("\n  Party Distribution:")
    for party, count in people_df['party'].value_counts().items():
        percentage = (count / len(people_df)) * 100
        print(f"    â€¢ {party}: {count:,} ({percentage:.1f}%)")
    
    print("\n  Top 10 States by Legislator Count:")
    for i, (state, count) in enumerate(people_df['state'].value_counts().head(10).items(), 1):
        print(f"    {i:2d}. {state}: {count:,} legislators")
    
    print("\n  Sponsorship Roles:")
    for role, count in sponsorships_df['role'].value_counts().items():
        print(f"    â€¢ {role.title()}: {count:,} instances")
    
    # === COMMITTEES SUMMARY ===
    if 'committees' in data_dict and len(data_dict['committees']) > 0:
        committees_df = data_dict['committees']
        print("\nðŸ›ï¸  COMMITTEES")
        print("-" * 80)
        print(f"Total Committee Assignments: {len(committees_df):,}")
        print(f"\n  Top 10 Most Active Committees:")
        for i, (committee, count) in enumerate(committees_df['committee_name'].value_counts().head(10).items(), 1):
            print(f"    {i:2d}. {committee}: {count:,} bills")
    
    # === TEXT VERSIONS SUMMARY ===
    if 'text_versions' in data_dict and len(data_dict['text_versions']) > 0:
        text_versions_df = data_dict['text_versions']
        print("\nðŸ“„ BILL TEXT VERSIONS")
        print("-" * 80)
        print(f"Total Text Versions Available: {len(text_versions_df):,}")
        print(f"\n  Version Types:")
        for version_type, count in text_versions_df['version_type'].value_counts().head(10).items():
            print(f"    â€¢ {version_type}: {count:,}")
    
    print("\n" + "=" * 80)
    print("END OF SUMMARY")
    print("=" * 80)
```

## Download/Parse bill data

### house 117th

```{python}
house_117 = download_congress_bills(117, ["hr"])
house_117_dfs = process_bills_to_dataframes(house_117)
summary_h_117 = generate_summary_stats(house_117_dfs)
```

```{python}
# save dataframes
print("\nSaving to pickle files...")
house_117_dfs['bills'].to_pickle(TEMP_DIR / 'house_117_bills.pkl')
house_117_dfs['people'].to_pickle(TEMP_DIR / 'house_117_people.pkl')
house_117_dfs['actions'].to_pickle(TEMP_DIR / 'house_117_actions.pkl')
house_117_dfs['sponsorships'].to_pickle(TEMP_DIR / 'house_117_sponsorships.pkl')
house_117_dfs['committees'].to_pickle(TEMP_DIR / 'house_117_committees.pkl')
```

```{python}
print("\n=== Download Verification ===")
for bill_type, files in house_117.items():
    print(f"{bill_type}: {len(files)} files downloaded")
    # Check file sizes
    total_size = sum(Path(f).stat().st_size for f in files) / (1024**2)  # MB
    print(f"  Total size: {total_size:.2f} MB")
```

### house 118th

```{python}
house_118 = download_congress_bills(118, ["hr"])
house_118_dfs = process_bills_to_dataframes(house_118)
summary_h_118 = generate_summary_stats(house_118_dfs)
```

```{python}
# save dataframes
print("\nSaving to pickle files...")
house_118_dfs['bills'].to_pickle(TEMP_DIR / 'house_118_bills.pkl')
house_118_dfs['people'].to_pickle(TEMP_DIR / 'house_118_people.pkl')
house_118_dfs['actions'].to_pickle(TEMP_DIR / 'house_118_actions.pkl')
house_118_dfs['sponsorships'].to_pickle(TEMP_DIR / 'house_118_sponsorships.pkl')
house_118_dfs['committees'].to_pickle(TEMP_DIR / 'house_118_committees.pkl')
```

```{python}
print("\n=== Download Verification ===")
for bill_type, files in house_118.items():
    print(f"{bill_type}: {len(files)} files downloaded")
    # Check file sizes
    total_size = sum(Path(f).stat().st_size for f in files) / (1024**2)  # MB
    print(f"  Total size: {total_size:.2f} MB")
```

## check

```{python}
house_118_dfs['actions']['action_text'].value_counts().head(10)
```

```{python}
pd.crosstab(house_118_dfs['sponsorships']['role'], house_118_dfs['sponsorships']['is_original_cosponsor'], dropna = False)
```

# Secondary Data Sources

## Legislative effectiveness

### Why Legislative Effectiveness Scores?

The most credible source of metric for performance of legislative activity is "Legislative Effectiveness Score (LES)" developed by political scientists Craig Volden and Alan Wiseman - who lead the Center for Effective Lawmaking and share panel data on LES score.

LES scores provide validated performance metrics developed by political scientists. These scores capture a legislator's ability to advance bills through the legislative process, offering a standardized measure of effectiveness that's comparable across members and sessions.

The center shares processed data with minimal cleaning and the format is in .dta (stata) at this link <https://thelawmakers.org/data-download>. The dataset contains `bioguide_id` as a 'key' link with other dataset.

We use the **reduced dataset** which focuses on key variables while maintaining the essential metrics needed for our analysis.

```{python}
# read data
les_raw_df = pd.read_stata(Path(RAW_DATA_DIR/ 'les/CELHouse93to118Reduced-REVISED-06.26.2025.dta' ))
```

```{python}
# filter relevant variables
les_df = les_raw_df.filter(['icpsr', 'bioname', 'bioguide_id', 
    'female', 'afam', 'latino',
    'state_leg',
    'elected', 'votepct
    'party_code', 'seniority','freshman',  'speaker','chair',
    'congress', 'year', 'st_name', 'cd', 
    'dwnom1', 'dwnom2',
    'les2', 'expectation2', 'allbill2', 'allpass2', 'allaic2', 'allabc2', 'all law2'])
```

```{python}
# filter out working dataset for the focused time periods
les_df = les_df.query ("congress == 117| congress == 118")
```

```{python}
# save filtered working dataframe into temp folder
les_df.to_pickle(Path(TEMP_DIR/ "les_df.pkl"))
```

## Historical list of Legislators

The list of legislators is downloaded from github repository of "unitedstates" and the original format are in .csv.

```{python}
from io import StringIO
# Download and save legislators
# directly download csv file with pd.read_csv(file_url) did not work for this urls
current_response = requests.get("https://unitedstates.github.io/congress-legislators/legislators-current.csv")
current_legislators = pd.read_csv(StringIO(current_response.text))
current_legislators.to_csv(RAW_DATA_DIR / "current_legislators.csv", index=False)

historical_response = requests.get("https://unitedstates.github.io/congress-legislators/legislators-historical.csv")
historical_legislators = pd.read_csv(StringIO(historical_response.text))
historical_legislators.to_csv(RAW_DATA_DIR / "historical_legislators.csv", index=False)
```

```{python}
# append historical and current legislators together into a single dataframe
legislators = pd.concat([historical_legislators, current_legislators], ignore_index=True)

del(current_legislators, historical_legislators)
```

```{python}
# save the concatenated data into temp data folder
legislators.to_csv(TEMP_DIR/ "legislators.csv", index = False)
```

# Link between sources

```{python}
# Verify linkage across datasets
print("\n=== Cross-Dataset Linkage Check ===")

# Check bioguide_id overlap
bill_people = set(house_117_dfs['people']['bioguide_id']).union(set(house_118_dfs['people']['bioguide_id']))
les_people = set(les_df['bioguide_id'])
legislators_people = set(legislators['bioguide_id'])

print(f"Legislators in bill data: {len(bill_people)}")
print(f"Legislators in LES data: {len(les_people)}")
print(f"Legislators in historical list: {len(legislators_people)}")
print(f"\nOverlap (bills & LES): {len(bill_people & les_people)}")
print(f"Overlap (bills & historical): {len(bill_people & legislators_people)}")
```

# Summary

This notebook successfully acquired and staged data from three sources:

## Data Acquired

1.  **Bioguide profiles** - Biographical data for all members of Congress
    -   Saved as: `bioguide_raw.pkl`
    -   Records: \[insert count\]
2.  **Congressional bills** - Legislative data for 117th and 118th sessions
    -   House bills only (HR type)
    -   Saved as separate tables: bills, people, actions, sponsorships, committees
    -   Records: \[insert counts\]
3.  **Supplementary datasets**
    -   Legislative Effectiveness Scores: `les_df.pkl`
    -   Historical legislator list: `legislators.csv`

-   All datasets include `bioguide_id` for cross-linking

## Next Steps

The `02-data-cleaning` notebook will:

-   Standardize formats and variable names

-   Create derived variables for analysis

-   Merge datasets on common identifiers